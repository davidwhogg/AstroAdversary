% This file is part of the AstroAdversary project.
% Copyright 2019 the authors.

\documentclass[12pt, letterpaper]{article}

% page setup
\addtolength{\topmargin}{-0.75in}
\addtolength{\textheight}{1.50in}

% math shih
\newcommand{\teff}{{T_{\mathrm{eff}}}}
\newcommand{\logg}{\log g}
\newcommand{\feh}{\left[\mathrm{Fe/H}\right]}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing

\section*{\raggedright
          Adversarially attacking machine-learning
          methods in astrophysics}

\noindent
Villar, Hogg, others

\paragraph{abstract:}
% Context
In classification contexts, most machine-learning methods are
susceptible to adversarial attacks, in which obviously small or
uninformative changes to input data can be found that lead to substantial
changes in the output classifications of those data.
The success of these kinds of attacks undermines the idea that
these methods---which are becoming more prevalent---are valid for use
in scientific projects.
% Aims
In this contribution, we compare different general classes of
machine-learning methods in terms of their susceptibility to
adversarial attacks.
% Methods
We generalize the concept of an adversarial attack to the context of
regressions, which is the context of most machine-learning
applications in astrophysics.
We search for attacks against some popular classes of models in
astronomy, using examples from stellar spectroscopy for demonstration
purposes (although we expect the results to be general).
% Results
We find, as expected, that the standard practices of regression with
flexible machine-learning methods (such as deep learning), even with
good training data, are generally susceptible to impressive
adversarial attacks.
We find that methods in which the machine-learning method is used to
make a generative model for the data (from a latent space or label
space) are far less susceptible to such attacks, probably because they
conform more closely to our physical knowledge.
Even methods that are susceptible to attacks can be made less
susceptible by modifications to the model structure or to the training
regime.
% Conclusion
We comment on the relationships between adversarial attacks,
generalizability, interpretability, and causal structure.

\section{Introduction}

Hello World!

Introduce the concepts of the causal direction and the anti-causal
direction, and classify some papers and methods in this context.

\section{What is regression?}

In the standard regression setting in which machine learning has been
applied in astrophysics, there are $N$ training data examples $y_n$
(which could be images or spectra or light curves).
For each of these examples $n$, in addition to the data $y_n$ there is
a continuous label $\ell_n$ (which could be a vector or blob of
labels).
The goal of the machine-learning method is to learn or fit a complex
function $h(\cdot)$ from the data space $y$ to the label space $\ell$
such that the predictive accuracy (for, say, held-out data) is high.
The goal is usually achieved by optimizing some kind of objective
function that is some kind of norm (L2 often) of the differences
between the labels $\ell_n$ and the predictions $h(y_n)$ of those
labels.

This standard setting for regression is what we call
\emph{anti-causal}.
It is anti-causal because we are trying, in some sense, to infer the
causes (labels) of the data by transforming the data into the labels,
when it is closer to our beliefs to say that the data are generated by
the labels.
That is, the method finds a function from the data space to the label
space.
The only real constraints on this function are that it do a good job
of predicting labels.
This is anti-causal because we think it is closer to the truth
that the data are generated by a process that is a function that goes 
from the labels to the data.

In what follows, we will work with real-data and fake-data examples.
The fake data $y_n$ will be noisy measurements of a linear function,
at a set of control points.
The label $\ell_n$ for each fake data point will be the slope of that
linear relationship.
Some examples of these fake training data are shown in
\figurename~XXX, along with the distribution of the training data in
the label space.
The fake data are valuable because they are generated by such a simple
true process, and because there exists an efficient estimator
(minimum-variance unbiased estimator) for the labels.

The real data $y_n$ used in this project will be stellar spectra, or
a vector of continuum-normalized intensities at a set of wavelengths.
Some examples of the training data are shown in \figurename~YYY.
And here the labels $\ell_n$ will be stellar effective temperature
$\teff_n$, surface gravity $\logg_n$, and iron abundance $\feh_n$.
The distribution of the training data in these quantities is also
shown in \figurename~YYY.

In general, the point of a supervised machine-learning method is to
label unlabeled or held-out data.
That is, there is a set of $M$ data points $y_m$ that were not part of
the data used to train or validate the model.
These data points either don't have labels or else they do have labels
but they are only being used to test the predictive accuracy.
They are not used in training.
When we are working in the anti-causal direction, the prediction for
the labels $\ell_m$ for test-set data point $y_m$ is simply $h(y_m)$,
where the function $h(\cdot)$ was fit to, or trained on, the training
data.

When we employ machine-learning methods that run in the causal
direction, it looks a bit different:
Instead of trying to find a function $h(\cdot)$ that goes from the
data space to the label space, we try to find a function $g(\cdot)$
that goes from the label space to the data space.
In training, we optimize some kind of objective function
that is some kind of norm (L2 often) of the differences
between the data $y_n$ and the predictions $g(\ell_n)$ of data
from this generative or forward model.

This kind of causal-direction model transforms from labels to data,
but it can still be used to label test-set data points $y_m$ that
don't have labels.
Fundamentally, the prediction for the labels $\ell_m$ for test-set
data point $y_m$ is made by solving the inverse problem, or performing
inference:
We find labels $\ell_m$ that come close to generating the data $y_m$.
Again, we define ``come close'' in terms of some kind of norm (L2
often) of the differences between the data $y_m$ and the prediction
$g(\ell_m)$ of the data given putative labels $\ell_m$.

Importantly, in both cases (causal and anti-causal), there are training
data points $y_n$ with labels $\ell_n$ and there are test-set data points
$y_m$ (with no labels).
The model is trained by optimizing some kind of objective based on a
norm of the differences between training data or training labels and
predictions thereof.
The model is used on test-set data points $y_m$ by either computing or
inferring labels $\ell_m$ that are consistent with the data points
$y_m$ and the trained model.

\section{What is an adversarial attack?}

In both cases (causal and anti-causal), there is a label $\ell_m$ given
to each test-set data point $y_m$.



foo...

\end{document}
